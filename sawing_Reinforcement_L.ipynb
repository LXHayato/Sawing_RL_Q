{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sawing_Reinforcement_L.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOelUfBwPThD86MhG3nPY9q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LXHayato/Sawing_RL_Q/blob/def_Create_01/sawing_Reinforcement_L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mvP58QeWYR6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "outputId": "cd79c27f-5865-4197-feb7-80cf9f808034"
      },
      "source": [
        "#とりあえずカテドラルローブのみ\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import copy\r\n",
        "import random\r\n",
        "\r\n",
        "#変数の定義\r\n",
        "action = None\r\n",
        "reward = None\r\n",
        "\r\n",
        "#Q学習\r\n",
        "class QL_Agent:\r\n",
        "  def __init__(self, epsilon=0.1,alpha=.2, gamma=.99, actions=None, observation=None):\r\n",
        "    self.alpha=alpha\r\n",
        "    self.gamma=gamma\r\n",
        "    self.epsilon=epsilon\r\n",
        "    self.reward_history=[]\r\n",
        "    self.actions=actions\r\n",
        "    self.state=str(observation)\r\n",
        "    self.ini_state=str(observation)\r\n",
        "    self.previous_map=None\r\n",
        "    self.previous_action=None\r\n",
        "    self.q_values=self._init_q_values()\r\n",
        "\r\n",
        "#Qテーブルの初期化\r\n",
        "  def _init_q_values(self):\r\n",
        "    q_values={}\r\n",
        "    q_values[self.state]=np.repeat(0.0, len(self.actions))\r\n",
        "    return q_values\r\n",
        "\r\n",
        "#状態の初期化\r\n",
        "  def init_state(self):\r\n",
        "    self.previous_map=copy.deepcopy(self.ini_state)\r\n",
        "    self.state=copy.deepcopy(self.ini_state)\r\n",
        "    return self.state\r\n",
        "\r\n",
        "#ε-greedy選択\r\n",
        "  def act(self, nb_episode):\r\n",
        "#集中力の取り得る範囲に絞る\r\n",
        "    if Con <= 6:\r\n",
        "      Srange = 9\r\n",
        "    elif Con <=7:\r\n",
        "      Srange = 27\r\n",
        "    #elif Con <=8:\r\n",
        "      #Srange = \r\n",
        "    #ランダム行動\r\n",
        "    if episode == 0:\r\n",
        "      self.epsilon = 1\r\n",
        "    #elif episode > 3000:\r\n",
        "      #self.epsilon = 0\r\n",
        "    else:\r\n",
        "      self.epsilon = (np.log(nb_episode)-np.log(episode))/np.log(nb_episode)*(np.log(nb_episode)-np.log(episode))/np.log(nb_episode)\r\n",
        "    if np.random.uniform() < self.epsilon:\r\n",
        "      action=np.random.randint(0, len(self.actions))\r\n",
        "    #greedy行動\r\n",
        "    else:\r\n",
        "      action=np.argmax(self.q_values[self.state])\r\n",
        "\r\n",
        "    self.previous_action=action\r\n",
        "    return action\r\n",
        "\r\n",
        "#次の状態と報酬の観測\r\n",
        "  def observe(self, next_map, reward=None):\r\n",
        "    next_map=str(next_map)\r\n",
        "    #初めての状態であれば\r\n",
        "    if next_map not in self.q_values:\r\n",
        "      self.q_values[next_map]=np.repeat(0.0, len(self.actions))\r\n",
        "\r\n",
        "    self.previous_map=copy.deepcopy(self.state)\r\n",
        "    self.state=next_map\r\n",
        "\r\n",
        "    if reward is not None:\r\n",
        "      self.reward_history.append(reward)\r\n",
        "      self.learn(reward)\r\n",
        "    \r\n",
        "#Q値の更新\r\n",
        "  def learn(self, reward):\r\n",
        "    q=self.q_values[self.previous_map][self.previous_action]\r\n",
        "    max_q=max(self.q_values[self.state])\r\n",
        "    self.q_values[self.previous_map][self.previous_action]=q+(self.alpha*(reward+(self.gamma*max_q)-q))\r\n",
        "\r\n",
        "\r\n",
        "#World\r\n",
        "class World:\r\n",
        "  def __init__(self):\r\n",
        "    self.actions ={\r\n",
        "        \"UL\":0,\"UC\":1,\"UR\":2,\"CL\":3,\"CC\":4,\"CR\":5,\"DL\":6,\"DC\":7,\"DR\":8,#通常縫い\r\n",
        "        \"UCZ\":9,\"URZ\":10,\"CCZ\":11,\"CRZ\":12,#たすき縫い\r\n",
        "        \"ULS\":13,\"UCS\":14,\"CLS\":15,\"CCS\":16,#逆たすき縫い\r\n",
        "        \"Mental\":17,\"Shift\":18,#精神統一、縫いパワーシフト\r\n",
        "        \"ULW\":19,\"UCW\":20,\"URW\":21,\"CLW\":22,\"CCW\":23,\"CRW\":24,\"DLW\":25,\"DCW\":26,\"DRW\":27#2倍縫い\r\n",
        "    }\r\n",
        "\r\n",
        "#初期値\r\n",
        "#集中力はLv70(187)+光の針(45)を使用\r\n",
        "\r\n",
        "    self.Start_map=np.array([[30,0,15],\r\n",
        "                   [0,45,0],\r\n",
        "                   [30,0,15]])\r\n",
        "\r\n",
        "#布現在値\r\n",
        "    self.Now_map = copy.deepcopy(self.Start_map)\r\n",
        "\r\n",
        "#行動の実行\r\n",
        "#状態、報酬、ゴールしたかを返却\r\n",
        "\r\n",
        "  def step(self, action, n, Con):  \r\n",
        "    #Cloth=np.sum(abs(self.Now_map))\r\n",
        "    if n == 0:\r\n",
        "      Mental = False\r\n",
        "    n+=1\r\n",
        "\r\n",
        "#縫いパワー計算\r\n",
        "#普通→??→強い→弱い→最強\r\n",
        "    if Mental == True:\r\n",
        "      n -= 1\r\n",
        "      Mn += 1\r\n",
        "      if Mn == 3:\r\n",
        "        Mn = 0\r\n",
        "        Mental = False\r\n",
        "    if n % 5 == 1:\r\n",
        "      Pow = 1\r\n",
        "    elif n % 5 == 2 or Shift == True:\r\n",
        "      Pow = random.randint(1, 4) / 2\r\n",
        "      Shift = False\r\n",
        "    elif n % 5 == 3:\r\n",
        "      Pow = 1.5\r\n",
        "    elif n % 5 == 4:\r\n",
        "      Pow = 0.5\r\n",
        "    else:\r\n",
        "      Pow = 2\r\n",
        "\r\n",
        "#通常縫い\r\n",
        "    if action == self.actions[\"UL\"]:\r\n",
        "      self.Now_map[0][0]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"UC\"]:\r\n",
        "      self.Now_map[0][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"UR\"]:\r\n",
        "      self.Now_map[0][2]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"CL\"]:\r\n",
        "      self.Now_map[1][0]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"CC\"]:\r\n",
        "      self.Now_map[1][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"CR\"]:\r\n",
        "      self.Now_map[1][2]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"DL\"]:\r\n",
        "      self.Now_map[2][0]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"DC\"]:\r\n",
        "      self.Now_map[2][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "    elif action == self.actions[\"DR\"]:\r\n",
        "      self.Now_map[2][2]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 5\r\n",
        "#たすき縫い、逆たすき縫い\r\n",
        "    elif action == self.actions[\"UCZ\"]:\r\n",
        "      self.Now_map[0][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"URZ\"]:\r\n",
        "      self.Now_map[0][2]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"CCZ\"]:\r\n",
        "      self.Now_map[1][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"CRZ\"]:\r\n",
        "      self.Now_map[1][2]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"ULS\"]:\r\n",
        "      self.Now_map[0][0]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"UCS\"]:\r\n",
        "      self.Now_map[0][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"CLS\"]:\r\n",
        "      self.Now_map[1][0]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"CCS\"]:\r\n",
        "      self.Now_map[1][1]-=random.randint(12,18) * Pow\r\n",
        "      Con -= 7\r\n",
        "#精神統一、縫いパワーシフト\r\n",
        "    elif action == self.actions[\"Mental\"]:\r\n",
        "      Mental += 3\r\n",
        "      MPow = Pow\r\n",
        "      Con -= 7\r\n",
        "    elif action == self.actions[\"Shift\"]:\r\n",
        "      Con -= 7\r\n",
        "      Shift = True\r\n",
        "#2倍縫い\r\n",
        "    elif action == self.actions[\"ULW\"]:\r\n",
        "      self.Now_map[0][0]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"UCW\"]:\r\n",
        "      self.Now_map[0][1]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"URW\"]:\r\n",
        "      self.Now_map[0][2]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"CLW\"]:\r\n",
        "      self.Now_map[1][0]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"CCW\"]:\r\n",
        "      self.Now_map[1][1]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"CRW\"]:\r\n",
        "      self.Now_map[1][2]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"DLW\"]:\r\n",
        "      self.Now_map[2][0]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"DCW\"]:\r\n",
        "      self.Now_map[2][1]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "    elif action == self.actions[\"DRW\"]:\r\n",
        "      self.Now_map[2][2]-=random.randint(12,18)*2 * Pow\r\n",
        "      Con -= 9\r\n",
        "\r\n",
        "#エピソード終了の確認\r\n",
        "\r\n",
        "    Cloth=np.sum(abs(self.Now_map))\r\n",
        "    is_goal = self._is_end_episode(Cloth,Con)\r\n",
        "    reward = self._compute_reward(Cloth, n, is_goal)\r\n",
        "    if episode % 500 == 0:\r\n",
        "      if is_goal == True:\r\n",
        "        print(f'{self.Now_map}')\r\n",
        "    return self.Now_map, reward, is_goal, n, Con\r\n",
        "\r\n",
        "#報酬\r\n",
        "  def _compute_reward(self, Cloth, n, is_goal):\r\n",
        "    if is_goal == False:\r\n",
        "      return 0\r\n",
        "    elif Cloth <= 8:\r\n",
        "      return 1\r\n",
        "    else:\r\n",
        "      return -1\r\n",
        "\r\n",
        "\r\n",
        "#終了条件の確認\r\n",
        "#集中力が4以下、もしくは残り値が8以下で終了\r\n",
        "  def _is_end_episode(self,Cloth,Con):\r\n",
        "   \r\n",
        "    if Con < 4:\r\n",
        "      n = 0\r\n",
        "      return True\r\n",
        "    elif Cloth <= 8:\r\n",
        "      n = 0\r\n",
        "      return True\r\n",
        "    else:\r\n",
        "      return False\r\n",
        "\r\n",
        "  def reset(self):\r\n",
        "    self.Now_map=copy.deepcopy(self.Start_map)\r\n",
        "    return self.Start_map\r\n",
        "\r\n",
        "#if __name__ =='__main':\r\n",
        "grid_env=World()\r\n",
        "ini_state=grid_env.Start_map\r\n",
        "agent = QL_Agent(epsilon=0.1,actions=np.arange(18),observation=ini_state)\r\n",
        "nb_episode = 5\r\n",
        "episode_No = [0.0 for i in range(int(nb_episode/100))]\r\n",
        "Success = np.array([0.0 for i in range(int(nb_episode/100))])\r\n",
        "rewards = []\r\n",
        "is_end_episode = False\r\n",
        "for episode in range(nb_episode):\r\n",
        "  if episode % 100 == 0:\r\n",
        "    print(f'今は{episode}周目')\r\n",
        "  episode_reward = []\r\n",
        "  n = 0\r\n",
        "  Con = 50\r\n",
        "  while(is_end_episode == False):\r\n",
        "    action = agent.act(nb_episode)\r\n",
        "    state, reward, is_end_episode, n, Con = grid_env.step(action, n, Con)\r\n",
        "    agent.observe(state, reward)\r\n",
        "    episode_reward.append(reward)\r\n",
        "  rewards.append(np.sum(episode_reward))\r\n",
        "  state = grid_env.reset()\r\n",
        "  agent.observe(state)\r\n",
        "  is_end_episode = False\r\n",
        "  if np.sum(episode_reward) >= 1:\r\n",
        "    Success[int(episode/100)] += 1\r\n",
        "\r\n",
        "#結果のプロット\r\n",
        "#figure()でグラフを表示する領域をつくり，figというオブジェクトにする．\r\n",
        "fig = plt.figure(figsize=(12, 6))\r\n",
        "\r\n",
        "#add_subplot()でグラフを描画する領域を追加する．引数は行，列，場所\r\n",
        "ax1 = fig.add_subplot(1, 2, 1)\r\n",
        "ax2 = fig.add_subplot(1, 2, 2)\r\n",
        "\r\n",
        "y1 = np.arange(nb_episode)\r\n",
        "y2 = np.arange(0,nb_episode,100)\r\n",
        "\r\n",
        "c1,c2 = \"blue\",\"green\"  # 各プロットの色\r\n",
        "l1,l2 = \"reward\",\"Success Rate\"   # 各ラベル\r\n",
        "\r\n",
        "ax1.plot(y1, rewards, color=c1, label=l1)\r\n",
        "ax2.plot(y2, np.array(Success/100)*100, color=c2, label=l2)\r\n",
        "ax1.legend(loc = 'upper right') #凡例\r\n",
        "ax2.legend(loc = 'upper right') #凡例\r\n",
        "fig.tight_layout()              #レイアウトの設定\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "今は0周目\n",
            "1\n",
            "False\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-3927e254210f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_end_episode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_end_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0mepisode_reward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-3927e254210f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, n, Con)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMental\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m#縫いパワー計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'Mental' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNScaxAj9d5g"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}